{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Overview of This Notebook**\n",
    "\n",
    "This notebook provides an introduction to LangChain, along with a few examples demonstrating how to use LangChain with AWS Bedrock models.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is LangChain?**\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications powered by large language models (LLMs).\n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits of Using LangChain**\n",
    "\n",
    "1. Offers a comprehensive set of tools to build LLM-based applications.\n",
    "2. Provides common, reusable functionalities such as prompt generation.\n",
    "3. Enables seamless integration with external data sources, databases, and third-party tools.\n",
    "4. Supports experimentation with different LLMs, allowing users to evaluate and choose models based on specific performance requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### **Alternative to LangChain**\n",
    "\n",
    "* **Together AI** – A platform that hosts a wide range of language models in one centralized location, providing an alternative approach to building LLM-based solutions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Tools Supported in LangChain**\n",
    "\n",
    "1. **LangSmith** – A tool for tracing and evaluating LLM applications and intelligent agents, helping developers transition from prototyping to production.\n",
    "2. **LangGraph** – Used to build stateful, multi-agent applications with LLMs. While it integrates seamlessly with LangChain, it can also function independently. LangGraph is trusted by companies like LinkedIn, Uber, Klarna, GitLab, and others for production-level deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Required Dependencies**\n",
    "\n",
    "To run this notebook, ensure that all necessary dependencies are installed.\n",
    "\n",
    "If you plan to use AWS Bedrock models, make sure your AWS credentials are configured beforehand. You can set them up using environment variables as described in the AWS CLI documentation:\n",
    "🔗 [Configure AWS Credentials](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-envvars.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While running in vscode, open the terminal in vscode, navigate to ~/.aws/credentials, copy the credentials under the default and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting awscli\n",
      "  Using cached awscli-1.40.43-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: langchain in /opt/miniconda3/lib/python3.13/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-aws in /opt/miniconda3/lib/python3.13/site-packages (0.2.27)\n",
      "Requirement already satisfied: botocore==1.38.44 in /opt/miniconda3/lib/python3.13/site-packages (from awscli) (1.38.44)\n",
      "Collecting docutils<=0.19,>=0.18.1 (from awscli)\n",
      "  Downloading docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/miniconda3/lib/python3.13/site-packages (from awscli) (0.13.0)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /opt/miniconda3/lib/python3.13/site-packages (from awscli) (6.0.2)\n",
      "Collecting colorama<0.4.7,>=0.2.5 (from awscli)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/miniconda3/lib/python3.13/site-packages (from botocore==1.38.44->awscli) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/miniconda3/lib/python3.13/site-packages (from botocore==1.38.44->awscli) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/miniconda3/lib/python3.13/site-packages (from botocore==1.38.44->awscli) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.38.44->awscli) (1.17.0)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /opt/miniconda3/lib/python3.13/site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/miniconda3/lib/python3.13/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/miniconda3/lib/python3.13/site-packages (from langchain) (0.4.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/miniconda3/lib/python3.13/site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/lib/python3.13/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: boto3>=1.37.24 in /opt/miniconda3/lib/python3.13/site-packages (from langchain-aws) (1.38.44)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in /opt/miniconda3/lib/python3.13/site-packages (from langchain-aws) (2.3.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/miniconda3/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Downloading awscli-1.40.43-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyasn1, docutils, colorama, rsa, awscli\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [awscli]2m4/5\u001b[0m [awscli]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed awscli-1.40.43 colorama-0.4.6 docutils-0.19 pyasn1-0.6.1 rsa-4.7.2\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install awscli langchain langchain-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the environment set up and all dependencies installed, we're ready to start loading models and exploring LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Overview of LangChain Chat Models**\n",
    "\n",
    "LangChain's chat models offer a convenient interface to interact with modern large language models (LLMs). These models come with enhanced capabilities that go beyond simple text generation.\n",
    "\n",
    "#### **Key Capabilities of Chat Models**\n",
    "\n",
    "1. Tool invocation (e.g., calling APIs or functions)\n",
    "2. Structured output generation\n",
    "3. Support for multimodal inputs (e.g., text, images)\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Features of LangChain**\n",
    "\n",
    "1. Seamless integration with various chat models\n",
    "2. Built-in message formatting using LangChain’s standardized prompt structure\n",
    "3. Tool calling API for dynamic function execution\n",
    "4. Support for structured and typed outputs\n",
    "5. Efficient batching, asynchronous programming, and a robust streaming API\n",
    "6. Integration with **LangSmith** for monitoring and debugging\n",
    "7. Built-in rate limiting and caching mechanisms to optimize performance\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Various Methods to Interact with Chat Models**\n",
    "\n",
    "There are multiple ways to retrieve responses from chat models. Each method should be implemented to explore its functionality:\n",
    "\n",
    "1. `invoke` – Standard method to get a single response\n",
    "2. `stream` – Stream responses in real time\n",
    "3. `batch` – Process multiple inputs in parallel\n",
    "4. `bind_tools` – Use tools/functions in combination with the model\n",
    "5. Structured Output – Generate and parse structured responses (e.g., JSON)\n",
    "\n",
    "For more details and additional methods, refer to the LangChain API documentation:\n",
    "🔗 [LangChain BaseChatModel Reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your AWS credentials are configured\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': '08749c96-e8a5-4593-ba9e-675f98ea649f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 26 Jun 2025 03:14:04 GMT', 'content-type': 'application/json', 'content-length': '185', 'connection': 'keep-alive', 'x-amzn-requestid': '08749c96-e8a5-4593-ba9e-675f98ea649f'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [424]}, 'model_name': 'anthropic.claude-3-5-sonnet-20240620-v1:0'}, id='run--f5012e57-268e-4912-a6d6-f91d8c58eb2a-0', usage_metadata={'input_tokens': 17, 'output_tokens': 7, 'total_tokens': 24, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Prompt Templates**\n",
    "\n",
    "LangChain provides support for creating prompt templates that are structured and ready to be passed directly into a language model.\n",
    "\n",
    "For more details, refer to the documentation:\n",
    "🔗 [ChatPromptTemplate Reference](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "prompt_templates = read_data(\"prompt_templates.json\")\n",
    "query_data = read_data(\"query_data.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(key, parameters):\n",
    "    # Load the prompt template from the JSON file\n",
    "    global prompt_templates\n",
    "    lc_prompt_template = ChatPromptTemplate.from_messages(prompt_templates[key])\n",
    "    prompt = lc_prompt_template.invoke(parameters)\n",
    "    return prompt\n",
    "\n",
    "def invoke_model(model, key=\"summarization_prompts\", parameters=None):\n",
    "    \"\"\"\n",
    "    Invoke the model with a specific prompt key and parameters.\n",
    "    \"\"\"\n",
    "    prompt = get_prompt(key, parameters)\n",
    "    response = model.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "# def invoke_summarization(model):\n",
    "#     global query_data\n",
    "#     prompt = get_prompt(\"summarization_prompts\", {\"text\": query_data[\"summarization_text\"]})\n",
    "#     response = model.invoke(prompt)\n",
    "#     return response\n",
    "\n",
    "# def invoke_information_extraction(model):\n",
    "#     global query_data\n",
    "#     prompt = get_prompt(\"information_extraction_prompts\", {\"text\": query_data[\"information_extraction_text\"]})\n",
    "#     response = model.invoke(prompt)\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'OKT3 was originally sourced from mice.'\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(invoke_model(model, \"question_answering_prompts\", {\"text\": query_data[\"question_answering_text\"], \"question\": query_data[\"question_answering_question\"]}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "1. Learned how to import models using LangChain and directly invoke them with queries to retrieve responses.\n",
    "2. Gained an understanding of LangChain’s message format, including `HumanMessage` and `SystemMessage`.\n",
    "3. Explored various methods for providing context to the model through prompting.\n",
    "4. Learned how to use LangChain's prompt templating to craft more effective and structured prompts for improved model performance.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **More with LangChain**\n",
    "\n",
    "Next, we’ll explore another approach to prompting that helps generate structured output from the model. To achieve this, we’ll use tools such as `BaseModel` and `Field` from the **Pydantic** library.\n",
    "\n",
    "Let’s dive deeper into how this works.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification(semantic_label='negative', agression_score=4.0, language='en')\n",
      "Semantic Label: negative\n",
      "Aggression Score: 4.0\n",
      "Language: en\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Ensure your AWS credentials are configured\n",
    "# using init_chat_model, initializing the aws bedrock anthropic model\n",
    "model = init_chat_model(\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\")\n",
    "\n",
    "# Inorder to use structured output, we need to define a Pydantic model that matches the expected output structure.\n",
    "# Steps to create a structured output model:\n",
    "# Define a class that inherits from BaseModel and then define the fields with appropriate types and constraints.\n",
    "# Use the `with_structured_output` method to wrap the model with the structured output capabilities.\n",
    "class Classification(BaseModel):\n",
    "  semantic_label: str = Field(..., description=\"The semantic label of the classification\", enum=[\"positive\", \"negative\", \"neutral\"])\n",
    "  agression_score: float = Field(..., description=\"The aggression score of the classification, between 0 and 5\", ge=0, le=5)\n",
    "  language: str = Field(..., description=\"The language of the classification\", enum=[\"en\", \"it\", \"fr\", \"de\", \"es\"])\n",
    "\n",
    "structured_modal = model.with_structured_output(Classification)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "  \"Extract the desired information from the following text: {text}. \" \\\n",
    "  \"Only extract the properties of the Classification class.\"\n",
    ")\n",
    "lc_prmopt = prompt_template.invoke({\"text\": \"I am so angry that i don't want to meet you at all from the things that you did to me.\"})\n",
    "\n",
    "structured_response = structured_modal.invoke(lc_prmopt)\n",
    "\n",
    "pprint.pprint(structured_response)\n",
    "print(f\"Semantic Label: {structured_response.semantic_label}\")\n",
    "print(f\"Aggression Score: {structured_response.agression_score}\")\n",
    "print(f\"Language: {structured_response.language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
