{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ee3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import botocore\n",
    "from IPython.display import display, Markdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3898f",
   "metadata": {},
   "source": [
    "Langgraph is a framework to build llm agents, it provide lot of tools like way conencting to deployed models, options to create agents with different optimization techniques in terms of memory, human in loop etc.,\n",
    "\n",
    "In this notebook I am trying to use aws bedrock completely to communicate with the models in aws bedrock, in this way it provide me access to different aws services that can be utilized\n",
    "\n",
    "Initially it will be simple implemention where I should be able to query the llm in bedrock and later will try to incorporate more features like langchain prompt techniqes etc to get the better response from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b58afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the bedrock client\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region =  session.region_name\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02929740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.BedrockRuntime at 0x10a1aca70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e35b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the different available models\n",
    "\n",
    "MODELS = {\n",
    "  \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "  \"Claude 3.5 Sonnet\": \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "  \"Claude 3.5 Haiku\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "  \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",\n",
    "  \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",\n",
    "  \"DeepSeek-R1\": \"us.deepseek.r1-v1:0\",\n",
    "  \"Meta Llama 3.1 70B Instruct\": \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cccc97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to display the model response\n",
    "def display_response(response, model_name = None):\n",
    "    if model_name:\n",
    "      display(Markdown(f\"### Response from **{model_name}**:\\n\\n\"))\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\"+\"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0b0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarization using bedrock invoke api\n",
    "# otehr ways to use the API is converse API which is recommended\n",
    "\n",
    "text_to_summarize = \"\"\"\n",
    "  AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
    "  a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
    "  Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
    "  democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
    "  for text and images—including Amazons Titan FMs, which consist of two new LLMs we're also announcing \\\n",
    "  today—through a scalable, reliable, and secure AWS managed service. With Bedrock's serverless experience, \\\n",
    "  customers can easily find the right model for what they're trying to get done, get started quickly, privately \\\n",
    "  customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
    "  tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
    "  with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ed2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"summairize the information in the following text\"\n",
    "<text>\n",
    "{text_to_summarize}\n",
    "</text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a960e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_body = json.dumps({\n",
    "  \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "  \"max_tokens\": 1000,\n",
    "  \"temperature\": 0.5,\n",
    "  \"top_p\": 0.9,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "    }\n",
    "  ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d2d331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from **Claude 3.7 Sonnet (Invoke Model API)**:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Amazon Bedrock Announcement Summary\n",
       "\n",
       "AWS has announced Amazon Bedrock, a new service that provides API access to foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon itself. Key features include:\n",
       "\n",
       "- Easy access to powerful text and image foundation models, including Amazon's new Titan LLMs\n",
       "- Serverless experience with no infrastructure management required\n",
       "- Ability for customers to privately customize models with their own data\n",
       "- Integration with existing AWS tools like SageMaker ML features\n",
       "- Support for testing different models and managing FMs at scale\n",
       "\n",
       "The service aims to democratize access to generative AI by making it easier for all builders to develop and scale AI-based applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        body=claude_body,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    \n",
    "    # Extract and display the response text\n",
    "    claude_summary = response_body[\"content\"][0][\"text\"]\n",
    "    display_response(claude_summary, \"Claude 3.7 Sonnet (Invoke Model API)\")\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "            \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "            \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "            \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797cc07e",
   "metadata": {},
   "source": [
    "Invoke api genrally ahs direct access to the models but still has many limitations\n",
    "\n",
    "1. Input/Output format is different\n",
    "2. no built-in support for multi conversations\n",
    "3. custom hadling for different model capabilities\n",
    "\n",
    "In below I am implementing the same summarization using converse api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "770a8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "converse_request = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": f\"Please provide a concise summary of the following text in 2-3 sentences. Text to summarize: {text_to_summarize}\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"inferenceConfig\": {\n",
    "    \"maxTokens\": 1000,\n",
    "    \"temperature\": 0.5,\n",
    "    \"topP\": 0.9\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b266954b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from **Claude 3.7 Sonnet (Converse API)**:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is a new service that provides API access to foundation models from various AI companies, including Amazon's own Titan models, enabling developers to build generative AI applications. It offers a serverless experience where customers can select, customize, and deploy models without managing infrastructure, making advanced AI capabilities more accessible to all builders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        messages=converse_request[\"messages\"],\n",
    "        inferenceConfig=converse_request[\"inferenceConfig\"]\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response\n",
    "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
    "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd3c38",
   "metadata": {},
   "source": [
    "Converse api supports the following parameters while generating a request\n",
    "1. modelId - name of the model\n",
    "2. messages - conversational messages/history\n",
    "3. toolsConfig - list of the tools that model can trigger to generate the output\n",
    "4. inference config\n",
    "\n",
    "Documentation - https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "861abda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_turn_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"Please summarize this text: {text_to_summarize}\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": claude_converse_response}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Can you make this summary even shorter, just 1 sentence?\"}]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be1452b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from **Claude 3.7 Sonnet (Multi-turn conversation)**:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is a new serverless service that provides API access to foundation models from multiple AI companies, allowing developers to easily build, customize, and deploy generative AI applications without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        messages=multi_turn_messages,\n",
    "        inferenceConfig={\"temperature\": 0.2, \"maxTokens\": 500}\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response using the correct structure\n",
    "    follow_up_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(follow_up_response, \"Claude 3.7 Sonnet (Multi-turn conversation)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928bca0",
   "metadata": {},
   "source": [
    "Till now we saw invoking the model directly using bedrock.invoke, as it has own limitations started implementing converse api but still it has some limitations majorly in converse api we need to wait until the whole response from model gets generated which is kind of not user friendly\n",
    "\n",
    "So bedrock supports conversestream api which helps in receiving the content as its being generated, it supports streaming so long generation response can be received incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aa294ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_converse(model_id, messages, inference_config=None):\n",
    "    if inference_config is None:\n",
    "        inference_config = {}\n",
    "    \n",
    "    print(\"Streaming response (chunks will appear as they are received):\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        response_stream = response.get('stream')\n",
    "        if response_stream:\n",
    "            for event in response_stream:\n",
    "\n",
    "                if 'messageStart' in event:\n",
    "                    print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "                if 'contentBlockDelta' in event:\n",
    "                    print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "                if 'messageStop' in event:\n",
    "                    print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "                if 'metadata' in event:\n",
    "                    metadata = event['metadata']\n",
    "                    if 'usage' in metadata:\n",
    "                        print(\"\\nToken usage\")\n",
    "                        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                        print(\n",
    "                            f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                        print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                    if 'metrics' in event['metadata']:\n",
    "                        print(\n",
    "                            f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "                \n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "        return full_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in streaming: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ce9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_request = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": f\"\"\"Please provide a detailed summary of the following text, explaining its key points and implications:\n",
    "                \n",
    "                {text_to_summarize}\n",
    "                \n",
    "                Make your summary comprehensive but clear.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41e2acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response (chunks will appear as they are received):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Role: assistant\n",
      "# Summary: Amazon Bedrock Announcement\n",
      "\n",
      "## Key Points\n",
      "\n",
      "Amazon Bedrock is a new AWS service that provides API access to foundation models (FMs) from multiple AI providers, including:\n",
      "- AI21 Labs\n",
      "- Anthropic\n",
      "- Stability AI\n",
      "- Amazon's own Titan models (newly announced)\n",
      "\n",
      "The service offers both text and image generation capabilities through a serverless, managed platform.\n",
      "\n",
      "## Core Features and Benefits\n",
      "\n",
      "1. **Democratized Access**: Makes powerful foundation models accessible to all developers without requiring specialized expertise.\n",
      "\n",
      "2. **Serverless Architecture**: Eliminates infrastructure management needs, allowing customers to focus on application development.\n",
      "\n",
      "3. **Model Selection**: Provides a range of models to choose from based on specific use case requirements.\n",
      "\n",
      "4. **Private Customization**: Allows customers to customize foundation models using their own proprietary data while maintaining privacy.\n",
      "\n",
      "5. **Integration with AWS Ecosystem**: \n",
      "   - Works with familiar AWS tools\n",
      "   - Integrates with SageMaker features like Experiments (for model testing)\n",
      "   - Connects with Pipelines for scaling model management\n",
      "\n",
      "6. **Enterprise-Grade Infrastructure**: Delivers scalability, reliability, and security through AWS's managed service approach.\n",
      "\n",
      "## Implications\n",
      "\n",
      "This announcement represents AWS's strategic response to customer demand for easier access to generative AI capabilities. By offering multiple models through a unified interface, AWS is positioning itself as an AI model aggregator and infrastructure provider rather than competing solely with its own models. The service lowers the barrier to entry for organizations wanting to implement generative AI while leveraging AWS's strengths in cloud infrastructure, security, and scalability.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 285\n",
      ":Output tokens: 356\n",
      ":Total tokens: 641\n",
      "Latency: 13669 milliseconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "streamed_response = stream_converse(\n",
    "    MODELS[\"Claude 3.7 Sonnet\"], \n",
    "    streaming_request, \n",
    "    inference_config={\"temperature\": 0.4, \"maxTokens\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
